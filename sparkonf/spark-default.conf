spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.driver.memory              8g
spark.executor.memory            8g
spark.memory.offHeap.enabled     true
spark.memory.offHeap.size        4g

spark.executor.extraJavaOptions  -Xms12g -Xmx12g



groupIdPrefix sparcala


kafka.bootstrap.servers localhost:9092
spark.kafka.bootstrap.servers localhost:9092

spark.security.credentials.kafka.enabled false

# A list of coma separated host/port pairs to use for establishing the initial connection to the Kafka cluster. For further details please see Kafka documentation. Only used to obtain delegation token.
spark.kafka.clusters.kafkartel.auth.bootstrap.servers localhost/9092

# Regular expression to match against the bootstrap.servers config for sources and sinks in the application. If a server address matches this regex, the delegation token obtained from the respective bootstrap servers will be used when connecting. If multiple clusters match the address, an exception will be thrown and the query won't be started. Kafka's secure and unsecure listeners are bound to different ports. When both used the secure listener port has to be part of the regular expression.
# spark.kafka.clusters.kafkartel.target.bootstrap.servers.regex .*

# Protocol used to communicate with brokers. For further details please see Kafka documentation. Protocol is applied on all the sources and sinks as default where bootstrap.servers config matches (for further details please see spark.kafka.clusters.kafkartel.target.bootstrap.servers.regex), and can be overridden by setting kafka.security.protocol on the source or sink.
spark.kafka.clusters.kafkartel.security.protocol PLAINTEXT

# The Kerberos principal name that Kafka runs as. This can be defined either in Kafka's JAAS config or in Kafka's config. For further details please see Kafka documentation. Only used to obtain delegation token.
# spark.kafka.clusters.kafkartel.sasl.kerberos.service.name kafka

# The file format of the trust store file. For further details please see Kafka documentation. Only used to obtain delegation token.
# spark.kafka.clusters.kafkartel.ssl.truststore.type None

# The location of the trust store file. For further details please see Kafka documentation. Only used to obtain delegation token.
# spark.kafka.clusters.kafkartel.ssl.truststore.location None

# The store password for the trust store file. This is optional and only needed if spark.kafka.clusters.kafkartel.ssl.truststore.location is configured. For further details please see Kafka documentation. Only used to obtain delegation token.
# spark.kafka.clusters.kafkartel.ssl.truststore.password None

# The file format of the key store file. This is optional for client. For further details please see Kafka documentation. Only used to obtain delegation token.
# spark.kafka.clusters.kafkartel.ssl.keystore.type None

# The location of the key store file. This is optional for client and can be used for two-way authentication for client. For further details please see Kafka documentation. Only used to obtain delegation token.
# spark.kafka.clusters.kafkartel.ssl.keystore.location None

# The store password for the key store file. This is optional and only needed if spark.kafka.clusters.kafkartel.ssl.keystore.location is configured. For further details please see Kafka documentation. Only used to obtain delegation token.
# spark.kafka.clusters.kafkartel.ssl.keystore.password None

# The password of the private key in the key store file. This is optional for client. For further details please see Kafka documentation. Only used to obtain delegation token.
# spark.kafka.clusters.kafkartel.ssl.key.password None

# SASL mechanism used for client connections with delegation token. Because SCRAM login module used for authentication a compatible mechanism has to be set here. For further details please see Kafka documentation (sasl.mechanism). Only used to authenticate against Kafka broker with delegation token.
# spark.kafka.clusters.kafkartel.sasl.token.mechanism SCRAM-SHA-512



spark.sql.execution.pythonUDF.arrow.enabled true



spark.sql.streaming.metricsEnabled true

spark.sql.streaming.checkpointLocation /tmp/checkpoint



# Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values spark.streaming.receiver.maxRate and spark.streaming.kafka.maxRatePerPartition if they are set (see below).
# spark.streaming.backpressure.enabled false

# This is the initial maximum receiving rate at which each receiver will receive data for the first batch when the backpressure mechanism is enabled.
# spark.streaming.backpressure.initialRate not set

# Interval at which data received by Spark Streaming receivers is chunked into blocks of data before storing them in Spark. Minimum recommended - 50 ms. See the performance tuning section in the Spark Streaming programming guide for more details.
# spark.streaming.blockInterval 200ms

# Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programming guide for mode details.
# spark.streaming.receiver.maxRate not set

# Enable write-ahead logs for receivers. All the input data received through receivers will be saved to write-ahead logs that will allow it to be recovered after driver failures. See the deployment guide in the Spark Streaming programming guide for more details.
# spark.streaming.receiver.writeAheadLog.enable false

# Force RDDs generated and persisted by Spark Streaming to be automatically unpersisted from Spark's memory. The raw input data received by Spark Streaming is also automatically cleared. Setting this to false will allow the raw data and persisted RDDs to be accessible outside the streaming application as they will not be cleared automatically. But it comes at the cost of higher memory usage in Spark.
# spark.streaming.unpersist true

# If true, Spark shuts down the StreamingContext gracefully on JVM shutdown rather than immediately.
spark.streaming.stopGracefullyOnShutdown true

# Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the new Kafka direct stream API. See the Kafka Integration guide for more details.
# spark.streaming.kafka.maxRatePerPartition not set

# Minimum rate (number of records per second) at which data will be read from each Kafka partition when using the new Kafka direct stream API.
spark.streaming.kafka.minRatePerPartition 1

# How many batches the Spark Streaming UI and status APIs remember before garbage collecting.
# spark.streaming.ui.retainedBatches 1000

# Whether to close the file after writing a write-ahead log record on the driver. Set this to 'true' when you want to use S3 (or any file system that does not support flushing) for the metadata WAL on the driver.
# spark.streaming.driver.writeAheadLog.closeFileAfterWrite false

# Whether to close the file after writing a write-ahead log record on the receivers. Set this to 'true' when you want to use S3 (or any file system that does not support flushing) for the data WAL on the receivers.
# spark.streaming.receiver.writeAheadLog.closeFileAfterWrite false
